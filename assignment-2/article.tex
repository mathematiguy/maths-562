%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	10pt, % Main document font size
	a4paper, % Paper type, use 'letterpaper' for US Letter paper
	oneside, % One page layout (no page indentation)
	%twoside, % Two page layout (page indentation for binding and different headers)
	headinclude,footinclude, % Extra spacing for the header and footer
	BCOR5mm, % Binding correction
]{scrartcl}
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\usepackage{mdframed}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays
\DeclareMathOperator*{\sig}{sig}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Caleb Moses*}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)


\newenvironment{problem}[2][]
{ \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
		{  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
{\textit{Solution:}}
{}

%----------------------------------------------------------------------------------------

\title{MATH/COMP 562 ASSIGNMENT 2}
\author{Caleb Moses}

\begin{document}

\maketitle

\section{Rademacher Complexity}

\begin{problem}{Exercise 1.1 (Rademacher complexity of linear hypothesis)}
Prove the following theorem.

\textbf{(Mohri) Theorem 5.10:} Define $B_r = \{x \in \mathbb{R}^d: ||x|| \leq r\}$. Let $S = \{x_1 , \ldots, x_m\} \subset X \subset \mathbb{R}^d$ with $X = B_r$. Consider the linear hypotheses $h(w, x) = w \cdot x$ and set $\mathcal{H} = \{h(x, w) = w \cdot x: x \in X, w \in B_\lambda\}$. Prove that the empirical Rademacher complexity is bounded as follows,
\[
	\hat{\mathfrak{R}}_S(H) \leq \frac{r\lambda}{\sqrt{m}}
\]
Hint: refer to class notes, Mohri textbook Theorem 5.10.
\end{problem}

\begin{solution}
	\begin{align}
		\hat{\mathfrak{R}}_S(\mathcal{H}) & = \frac{1}{m}\mathbb{E}_\sigma\left[ \sup_{||w||\leq \lambda}\sum_{i=1}^m\sigma_i w \cdot x_i \right] \text{ by definition}                                                                               \\
		                                  & = \frac{1}{m}\mathbb{E}_\sigma\left[ \sup_{||w||\leq \lambda} w\cdot \sum_{i=1}^m \sigma_i x_i \right] \text{ by linearity of the dot product}                                                            \\
		                                  & \leq \frac{1}{m}\mathbb{E}_\sigma\left[ \lambda \left|\left|\sum_{i=1}^m \sigma_i x_i \right|\right|\right] \text{ applying Cauchy-Schwartz and $||w|| \leq \lambda$}                                     \\
		                                  & \leq \frac{\lambda}{m}\mathbb{E}_\sigma\left[ \left|\left|\sum_{i=1}^m \sigma_i x_i \right|\right|\right] \text{ by the linearity of expectation}                                                         \\
		                                  & \leq \frac{\lambda}{m}\mathbb{E}_\sigma\left[ {\left(\left|\left|\sum_{i=1}^m \sigma_i x_i \right|\right|^2\right)}^\frac{1}{2}\right] \text{ since $||x|| = \sqrt{||x||^2}$}                             \\
		                                  & = \frac{\lambda}{m}\mathbb{E}_\sigma\left[ {\left(\sum_{i,j=1}^m \sigma_j \sigma_j (x_i \cdot x_j)\right)}^{\frac{1}{2}} \right] \text{since $||x||^2 = x^T x$ and rearranging terms}                     \\
		                                  & \leq \frac{\lambda}{m}{\left[\sum_{i=1}^m ||x_i||^2\right]}^{\frac{1}{2}} \text{ since $\mathbb{E}_\sigma[\sigma_i \sigma_j] = \mathbb{E}_\sigma[\sigma_i]\mathbb{E}_\sigma[\sigma_j] = 0$ for $i\neq j$} \\
		                                  & \leq \frac{\lambda \sqrt{mr^2}}{m} = \frac{r\lambda}{m} \text{ as required.}
	\end{align}
	Note that we are able to remove the expectation in line (7) since the dependency on $\sigma$ is removed.
\end{solution}

\begin{problem}{Exercise 1.2}
Given a hypothesis class $\mathcal{H}$, of functions $h : X \rightarrow \mathbb{R}$, and a dataset $S = \{x_1 , \ldots, x_m\} \subset X$. Define $\Phi(S) = \sup_{h\in \mathcal{H}} |\mathbb{E}[h] - \hat{\mathbb{E}}_S[h]|$ to be the least upper bound for the generalization gap of a function in $\mathcal{H}$.

Prove that $\hat{E}_S [\Phi(S)] \leq 2\hat{\mathfrak{R}}_m(\mathcal{H})$.
\end{problem}

\begin{solution}
	Let $S$ and $S^\prime$ be two datasets differing in one point, say $z_m\in S$ and $z_m^\prime \in S^\prime$. We proceed by bounding the expectation $\hat{\mathbb{E}}_S[\Phi(S)]$ as follows:
	\begin{align*}
		\hat{\mathbb{E}}_S[\Phi(S)] & = \hat{\mathbb{E}}_S\left[ \sup_{h\in\mathcal{H}} \left| \mathbb{E}[h] - \hat{\mathbb{E}}_S[h] \right| \right]                                                                                       \\
		                            & = \mathbb{E}_S \left[ \sup_{h\in\mathcal{H}} \hat{\mathbb{E}}_{S^\prime}[h] - \hat{\mathbb{E}}_{S}[h] \right]                                                                                        \\
		                            & \leq \mathbb{E}_{S,S^\prime}\left[ \sup_{h\in\mathcal{H}} \hat{\mathbb{E}}_{S^\prime}[h] - \hat{\mathbb{E}}_{S}[h] \right]                                                                           \\
		                            & = \mathbb{E}_{S,S^\prime} \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m\left(h(z_i^\prime) - h(z_i)\right) \right]                                                                          \\
		                            & = \mathbb{E}_{\sigma, S, S^\prime} \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i (h(z_i^\prime) - h(z_i)) \right]                                                                  \\
		                            & \leq \mathbb{E}_{\sigma, S^\prime}\left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sigma_i h(z_i^\prime) \right] + \mathbb{E}_{\sigma, S}\left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sigma_i h(z_i) \right] \\
		                            & = 2\mathbb{E}_{\sigma,S}\left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma h(z_i) \right] = 2\hat{\mathfrak{R}}_m(\mathcal{H})
	\end{align*}
\end{solution}

\begin{problem}{Exercise 1.3 (Rademacher Identities)}
Fix $m\geq 1$. Prove the following identities for any $\alpha \in \mathbb{R}$ and any two hypothesis sets $\mathcal{H}$ and $\mathcal{H^\prime}$ of functions mapping from $\mathcal{X}$ to $\mathcal{R}$.
\begin{enumerate}[label= (\alph*)]
	\item $\mathfrak{R}_m(\alpha \mathcal{H}) = |\alpha| \mathfrak{R}_m(\mathcal{H})$.
	\item $\mathfrak{R}_m(\mathcal{H} + \mathcal{H}^\prime) = \mathfrak{R}_m(\mathcal{H}) + \mathfrak{R}_m(\mathcal{H^\prime})$.
\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}[label= (\alph*)]
		\item We can bound $\mathfrak{R}_m(\alpha \mathcal{H})$ above and below and establish our result using the squeeze theorem.
		      \begin{align*}
			      \mathbb{E}_\sigma \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i (-|\alpha| h(z_i)) \right]                & \leq \mathfrak{R}_m(\alpha \mathcal{H}) & \leq \mathbb{E}_\sigma \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i (|\alpha| h(z_i)) \right] \\
			      \mathbb{E}_{\sigma^\prime} \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i^\prime (|\alpha| h(z_i)) \right] & \leq \mathfrak{R}_m(\alpha \mathcal{H}) & \leq \mathbb{E}_\sigma \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i (|\alpha| h(z_i)) \right]
		      \end{align*}
		      Both sides of the last inequality are equal to each other since $\sigma$ and $\sigma^\prime = -\sigma$ are identically distributed. Lastly we use linearity of expectation and the independence of $\alpha$ on $h$ to shift the $|\alpha|$ term outside of the expectation in the following way:
		      \begin{align*}
			      \mathfrak{R}_m(\alpha \mathcal{H}) & = \mathbb{E}_\sigma \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i (|\alpha| h(z_i)) \right] & = |\alpha| \mathbb{E}_\sigma \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i h(z_i) \right] \\
			                                         & = |\alpha| \mathfrak{R}_m(\mathcal{H}) \text{ this concludes the proof}
		      \end{align*}
		      \newpage
		\item We will rely on the fact that $h$ and $h^\prime$ do not depend on one another so maximising the sum is the same as maximising each separately. As a result, we can use the following argument: \begin{align*}
			      \mathfrak{R}_m(\mathcal{H} + \mathcal{H^\prime}) & = \mathbb{E}_\sigma \left[ \sup_{h\in\mathcal{H}, h^\prime \in \mathcal{H}^\prime} \frac{1}{m} \sum_{i=1}^m \sigma_i (h(z_i) + h^\prime (z_i)) \right]                                                                           \\
			                                                       & = \mathbb{E}_\sigma \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i (h(z_i)) \right] + \mathbb{E}_\sigma \left[ \sup_{h^\prime \in \mathcal{H}^\prime} \frac{1}{m} \sum_{i=1}^m \sigma_i (h^\prime(z_i)) \right] \\
			                                                       & = \mathfrak{R}_m(\mathcal{H}) + \mathfrak{R}_m(\mathcal{H}^\prime)
		      \end{align*}

	\end{enumerate}
\end{solution}
\section{Convex Learning Problems}
Refer to Ch 12 of Understanding Machine Learning (Shalev-Shwartz).

\begin{problem}{Exercise 1.4 (non convexity of 0\textendash{}1 loss)}
Construct an example showing that the 0\textendash{}1 loss function may suffer from local minima; namely, construct a training sample $S \in {(X \times \{\pm1\})}^m$ (say, for $X = \mathbb{R}^2$), for which there exist a vector $w$ and some $\varepsilon > 0$ such that:
\begin{enumerate}

	\item For any $w'$ such that $\|w - w'\| \leq \varepsilon$ we have $L_S (w) \leq L_S (w')$ (where the loss here is the 0\textendash{}1 loss). This means that $w$ is a local minimum of $L_S$.
	\item There exists some $w^*$ such that $L_S (w^*) < L_S (w)$. This means that $w$ is not a global minimum of $L_S$.
\end{enumerate}

Hint: Consider samples in a checkerboard pattern.
\end{problem}

\begin{solution}
	Consider three training points in $\mathbb{R}^2$:
	\[
		S = \{((0,0), -1), ((0,1), +1), ((1,0), +1)\}
	\]
	Let's select a vector $w=(0.5,0.5)$. Define the decision boundary as the hyperplane where $w \cdot x = 0.5$. This classifier $w$ has a loss of $L_S(w) = \frac{1}{3}$ because it misclassifies the point ((0,0),-1).

	Now, for a small $\varepsilon > 0$, consider the $\varepsilon$-ball around $w$. For any vector $w'$ within this ball (i.e., $|w - w'| \leq \varepsilon$), we observe that $L_S(w')$ will not be lower than $L_S(w)$. This is because moving the decision boundary within this $\varepsilon$-ball range won't help in correctly classifying the misclassified point. Therefore, the vector $w$ is a local minimum of $L_S$.

	However, consider another vector $w^\prime=(1,0)$ which defines the decision boundary as the hyperplane where $w^\prime \cdot x = 0.5$. This classifier $w^\prime$ correctly classifies all points in the training set S, and hence, its loss $L_S(w^\prime) = 0$. Therefore, we have $L_S(w^\prime) < L_S(w)$, meaning that the vector $w$ is not a global minimum of $L_S$.

	Thus, we have constructed a training sample S for which the 0\textendash{}1 loss function has a local minimum but not a global minimum. This satisfies all conditions of the problem.
\end{solution}

\begin{problem}{Exercise 1.5 (Convexity, Lipschitz, and Smoothness of logistic regression loss)}
Consider the learning problem of logistic regression: Let $H = X = \{x \in \mathbb{R}^d : \|x\| \leq B\}$, for some scalar $B > 0$, let $Y = \{\pm1\}$, and let the loss function $\ell$ be defined as $\ell(w, (x, y)) = \log(1 + \exp(-y\langle w, x \rangle))$. Show that the resulting learning problem is both convex-Lipschitz-bounded and convex-smooth-bounded. Specify the parameters of Lipschitzness and smoothness.
\end{problem}

\begin{solution}
	Given the logistic loss function
	\[\ell(w, (x, y)) = \log(1 + \exp(-y\langle w, x \rangle))\]

	The gradient of the loss function with respect to $w$ is
	\[\nabla \ell(w, (x, y)) = -\frac{y \exp(-y\langle w, x \rangle)}{1+\exp(-y\langle w, x \rangle)}x = -\frac{y}{1+\exp(y\langle w, x \rangle)}x\]

	Its norm can be bounded by $B$ because $\|x\| \leq B$. Thus, the loss function is B-Lipschitz.

	To evaluate smoothness, we look at the second derivative of the loss function with respect to $w$. The Hessian is given by
	\[\nabla^2 \ell(w, (x, y)) = \frac{\exp(y\langle w, x \rangle)}{(1+{\exp(y\langle w, x \rangle)})^2} x x^T \]

	The spectral norm of the Hessian can be bounded by $B^2$ as $\|x\|^2 \leq B^2$. Thus, the loss function is B²-smooth.

	Therefore, the logistic regression loss function is both B-Lipschitz and B²-smooth.
\end{solution}

\begin{problem}{Exercise 1.6 (Lipschitz continuity of the hinge loss)}
Consider the problem of learning halfspaces with the hinge loss. We limit our domain to the Euclidean ball with radius $R$. That is, $X = \{x : \|x\|_2 \leq R\}$. The label set is $Y = \{\pm1\}$ and the loss function $\ell$ is defined by $\ell(w, (x, y)) = \max\{0, 1 - y\langle w, x \rangle\}$. We already know that the loss function is convex. Show that it is $R$-Lipschitz.
\end{problem}

\begin{solution}

	A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is Lipschitz if there exists a constant $L > 0$ such that
	\[|f(u) - f(v)| \leq L\|u - v\|\]
	for all $u, v$ in the domain of $f$.

	Here, we are considering the hinge loss function, given by:
	\[\ell(w, (x, y)) = \max\{0, 1 - y\langle w, x \rangle\}\]

	To show that the hinge loss function is Lipschitz continuous, we have to prove that the difference in loss values for two distinct inputs $w$ and $w'$ is at most $R$ times the distance between $w$ and $w'$.

	Suppose $w, w' \in \mathbb{R}^d$ and $(x, y) \in X \times Y$, then:

	\begin{align*}
		| \ell(w, (x, y)) - \ell(w', (x, y)) | & = | \max\{0, 1 - y\langle w, x \rangle\} - \max\{0, 1 - y\langle w', x \rangle\} |                                               \\
		                                       & \leq | y\langle w, x \rangle - y\langle w', x \rangle | \quad \text{(since max function is Lipschitz with Lipschitz constant 1)} \\
		                                       & = | y\langle w - w', x \rangle |                                                                                                 \\
		                                       & \leq \| y(x) \|_2 \| w - w' \|_2 \quad \text{(by Cauchy-Schwarz inequality)}                                                     \\
		                                       & \leq R \| w - w' \|_2 \quad \text{(since $\| x \|_2 = R$)}
	\end{align*}

	Therefore, the hinge loss function $\ell$ is $R$-Lipschitz.
\end{solution}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%% \bibliography{sample.bib} % The file containing the bibliography

\end{document}
