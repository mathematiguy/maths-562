%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
  10pt, % Main document font size
  a4paper, % Paper type, use 'letterpaper' for US Letter paper
  oneside, % One page layout (no page indentation)
  %twoside, % Two page layout (page indentation for binding and different headers)
  headinclude,footinclude, % Extra spacing for the header and footer
  BCOR5mm, % Binding correction
]{scrartcl}
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\usepackage{mdframed}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays
\DeclareMathOperator*{\sig}{sig}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Caleb Moses*}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)


\newenvironment{problem}[2][]
{ \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
{\textit{Solution:}}
{}

%----------------------------------------------------------------------------------------

\title{MATH/COMP 562 ASSIGNMENT 2}
\author{Caleb Moses}

\begin{document}

\maketitle

\section{Rademacher Complexity}

\begin{problem}{Exercise 1.1 (Rademacher complexity of linear hypothesis)}
Prove the following theorem.

\textbf{(Mohri) Theorem 5.10:} Define $B_r = \{x \in \mathbb{R}^d: ||x|| \leq r\}$. Let $S = \{x_1 , \ldots, x_m\} \subset X \subset \mathbb{R}^d$ with $X = B_r$. Consider the linear hypotheses $h(w, x) = w \cdot x$ and set $\mathcal{H} = \{h(x, w) = w \cdot x: x \in X, w \in B_\lambda\}$. Prove that the empirical Rademacher complexity is bounded as follows,
\[
  \hat{\mathfrak{R}}_S(H) \leq \frac{r\lambda}{\sqrt{m}}
\]
Hint: refer to class notes, Mohri textbook Theorem 5.10.
\end{problem}

\begin{solution}
  \begin{align}
    \hat{\mathfrak{R}}_S(\mathcal{H}) & = \frac{1}{m}\mathbb{E}_\sigma\left[ \sup_{||w||\leq \lambda}\sum_{i=1}^m\sigma_i w \cdot x_i \right] \text{ by definition}                                                                               \\
                                      & = \frac{1}{m}\mathbb{E}_\sigma\left[ \sup_{||w||\leq \lambda} w\cdot \sum_{i=1}^m \sigma_i x_i \right] \text{ by linearity of the dot product}                                                            \\
                                      & \leq \frac{1}{m}\mathbb{E}_\sigma\left[ \lambda \left|\left|\sum_{i=1}^m \sigma_i x_i \right|\right|\right] \text{ applying Cauchy-Schwartz and $||w|| \leq \lambda$}                                     \\
                                      & \leq \frac{\lambda}{m}\mathbb{E}_\sigma\left[ \left|\left|\sum_{i=1}^m \sigma_i x_i \right|\right|\right] \text{ by the linearity of expectation}                                                         \\
                                      & \leq \frac{\lambda}{m}\mathbb{E}_\sigma\left[ {\left(\left|\left|\sum_{i=1}^m \sigma_i x_i \right|\right|^2\right)}^\frac{1}{2}\right] \text{ since $||x|| = \sqrt{||x||^2}$}                             \\
                                      & = \frac{\lambda}{m}\mathbb{E}_\sigma\left[ {\left(\sum_{i,j=1}^m \sigma_j \sigma_j (x_i \cdot x_j)\right)}^{\frac{1}{2}} \right] \text{since $||x||^2 = x^T x$ and rearranging terms}                     \\
                                      & \leq \frac{\lambda}{m}{\left[\sum_{i=1}^m ||x_i||^2\right]}^{\frac{1}{2}} \text{ since $\mathbb{E}_\sigma[\sigma_i \sigma_j] = \mathbb{E}_\sigma[\sigma_i]\mathbb{E}_\sigma[\sigma_j] = 0$ for $i\neq j$} \\
                                      & \leq \frac{\lambda \sqrt{mr^2}}{m} = \frac{r\lambda}{m} \text{ as required.}
  \end{align}
  Note that we are able to remove the expectation in line (7) since the dependency on $\sigma$ is removed.
\end{solution}

\begin{problem}{Exercise 1.2}
Given a hypothesis class $\mathcal{H}$, of functions $h : X \rightarrow \mathbb{R}$, and a dataset $S = \{x_1 , \ldots, x_m\} \subset X$. Define $\Phi(S) = \sup_{h\in \mathcal{H}} |\mathbb{E}[h] - \hat{\mathbb{E}}_S[h]|$ to be the least upper bound for the generalization gap of a function in $\mathcal{H}$.

Prove that $\hat{E}_S [\Phi(S)] \leq 2\hat{\mathfrak{R}}_m(\mathcal{H})$.
\end{problem}

\begin{solution}
  Let $S$ and $S^\prime$ be two datasets differing in one point, say $z_m\in S$ and $z_m^\prime \in S^\prime$. We proceed by bounding the expectation $\hat{\mathbb{E}}_S[\Phi(S)]$ as follows:
  \begin{align*}
    \hat{\mathbb{E}}_S[\Phi(S)] & = \hat{\mathbb{E}}_S\left[ \sup_{h\in\mathcal{H}} \left| \mathbb{E}[h] - \hat{\mathbb{E}}_S[h] \right| \right]                                                                                       \\
                                & = \mathbb{E}_S \left[ \sup_{h\in\mathcal{H}} \hat{\mathbb{E}}_{S^\prime}[h] - \hat{\mathbb{E}}_{S}[h] \right]                                                                                        \\
                                & \leq \mathbb{E}_{S,S^\prime}\left[ \sup_{h\in\mathcal{H}} \hat{\mathbb{E}}_{S^\prime}[h] - \hat{\mathbb{E}}_{S}[h] \right]                                                                           \\
                                & = \mathbb{E}_{S,S^\prime} \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m\left(h(z_i^\prime) - h(z_i)\right) \right]                                                                          \\
                                & = \mathbb{E}_{\sigma, S, S^\prime} \left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma_i (h(z_i^\prime) - h(z_i)) \right]                                                                  \\
                                & \leq \mathbb{E}_{\sigma, S^\prime}\left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sigma_i h(z_i^\prime) \right] + \mathbb{E}_{\sigma, S}\left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sigma_i h(z_i) \right] \\
                                & = 2\mathbb{E}_{\sigma,S}\left[ \sup_{h\in\mathcal{H}} \frac{1}{m} \sum_{i=1}^m \sigma h(z_i) \right] = 2\hat{\mathfrak{R}}_m(\mathcal{H})
  \end{align*}
\end{solution}

\section{Convex Learning Problems}
Refer to Ch 12 of Understanding Machine Learning (Shalev-Shwartz).

\subsection*{Exercise 1.4 (non convexity of 0\-1 loss)}
Problem 12.1. Hint: Consider samples in a checkerboard pattern.

\subsection*{Exercise 1.5 (Convexity, Lipschitz, and Smoothness of logistic regression loss)}
Problem 12.2.

\subsection*{Exercise 1.6 (Lipschitz continuity of the hinge loss)}
Problem 12.3.

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%% \bibliography{sample.bib} % The file containing the bibliography

\end{document}
