%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays
\DeclareMathOperator*{\sig}{sig}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Language reclamation: Literature review}} % The article title

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Caleb Moses*}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

\title{MATH/COMP 562 ASSIGNMENT 4}
\date{Due: April 9th (Sunday)}
\author{Adam M. Oberman}
\maketitle

\section*{Generative Models}
Refer to \url{https://udlbook.github.io/udlbook/}

\subsection*{Exercise 4.1 (GANs)}

Given a dataset of genuine images, $S_m = \{x_1, \ldots, x_m\}$. The goal is to train a generator, $x_i = g(z_i, \theta)$, to generate images $x_i \in S_m$ from noise $z_i$. This is achieved by also training a discriminator, $f(x, \phi)$, which classifies an image as genuine or generated, using the minimax training loss
\begin{equation*}
\max_\theta \min_\phi \mathcal{L}^S_\theta(f(x, \phi), y)
\end{equation*}
applied to the combined dataset, $S_\theta = S_N \cup S_G^\theta$,
where
\begin{itemize}
    \item $S_N = \{(x_1, 1), \ldots, (x_m, 1)\}$, the (fixed) genuine images, with label $y = 1$,
    \item $S_G^\theta = \{(x^\theta_1, 0), \ldots, (x^\theta_m, 0)\}$, a changing set of generated images $x^\theta_i = g(z_i, \theta)$, with label $y = 0$.
\end{itemize}

%% \subsubsection*{Explicit GAN loss}

%% From \citet{prince2023understanding} the GAN loss function is as follows:

%% %% This is the characterisation of the saddle points
%% \begin{equation*}
%% \hat{\theta} = \textrm{argmax}_\theta \left[ \min_\phi \left[ \sum_j - \log(1 - D(G(z_j, \theta), \phi))) - \sum_i \log(D(x_i, \phi))) \right] \right]
%% \end{equation*}

%% where $\theta$ represents the weights of the generator function $G$, and $\phi$ is the weights of the discriminator $D$, and $\sigma$ is the sigmoid logistic function.

\subsubsection*{Exercise 4.1 (a)}
Write down and explain the loss for the discriminator, if the generator is fixed.

\subsubsection*{Solution 4.1 (a)}

From \citet{prince2023understanding} 15.5 the discriminator loss function is given by:

\begin{equation*}
  L(\phi) = - \sum_j \log\left(1 - \textrm{sig}(f(g(z_j, \theta), \phi)\right) - \sum_i log\left(\textrm{sig}(f(x_i, \phi))\right)
\end{equation*}

where ``$\textrm{sig}$'' represents the sigmoid function.

For a fixed generator, we can imagine each $g(z_i, \theta) = \hat{x}_i$ where $\hat{x}_i$ is an image generated according to $g$ which approximates $x_i$. We can simplify further if we let $D(x, \phi) = \textrm{sig}(f(x, \phi))$, basically considering the sigmoid classification layer as part of the discriminator function.

This gives us the loss function below:

\begin{equation*}
  L(\phi) = - \sum_j \log(1 - D(\hat{x}_j, \phi)) - \sum_i log(D(x_i, \phi))
\end{equation*}

The result is a binary cross-entropy loss where the goal is to distinguish $x_i$ from $\hat{x}_i$. Real samples $x_i$ decrease the loss by $log(D(x_i, \phi))$ while generated samples increase the loss by $\log(1-D(\hat{x}_j, \phi))$.

\subsubsection*{Exercise 4.1 (b)}
Write down and explain the loss (or gain, since it is a maximization) for the generator, if the discriminator is fixed.

\subsubsection*{Solution 4.1 (b)}

From \citet{prince2023understanding} 15.5 the generator gain function is given by:

\begin{equation*}
  L(\theta) = - \sum_j \log\left(1 - \textrm{sig}(f(g(z_j, \theta), \phi)\right)
\end{equation*}

where, as before ``$\textrm{sig}$'' represents the sigmoid function.

For a fixed discriminator, we can let $D_\phi(x) = \textrm{sig}(f(x, \phi))$ (we are bringing the $\phi$ into the function, since now it can't vary). In this scenario the GAN problem requires us to find the optimal $\hat{\theta}$ satisfying below:

\begin{equation*}
  L(\theta) = - \sum_j \log(1 - D_{\phi}(g(z_j, \theta)))
\end{equation*}

The purpose of the generator gain function is to reward the generator for samples that are assigned high probability by the discriminator.

\subsubsection*{Exercise 4.1 (c)}
Characterize the saddle points of the loss. If the generator outputs the full set of genuine images, is this optimal? What if the generator outputs just a subset of the genuine images?

A saddle point, $(\theta, \phi)$, is a point where (i) if we fix $\phi$ and change $\theta$, we cannot further increase the loss, and (ii) if we fix $\theta$ and change $\phi$, we cannot further decrease the loss.

\subsubsection*{Solution 4.1 (c)}

The saddle points of the GAN loss function are solutions to the following minimax problem, as described in \citet{prince2023understanding} equation 15.4:

\begin{equation*}
\hat{\theta} = \argmax_\theta \left[ \min_\phi \left[ \sum_j - \log(1 - \sig(f(g(z_j, \theta), \phi))) - \sum_i \log(\sig(f(x_i, \phi))) \right] \right]
\end{equation*}

This represents the Nash equilibrium in the minimax game between the generator and the discriminator.

If the generator produces the full set of genuine images, the discriminator could learn to memorize the genuine images and achieve perfect discrimination. In this scenario the generator would not be optimal.

If however the generator only produces a subset of the genuine images, then it is sub-optimal. This phenomenon is referred to as as "mode collapse", where the generator learns to generate only a limited variety of samples.

\newpage

\subsection*{Exercise 4.2 (Diffusion generative models)}
Given the dataset $S_m = \{x_1 , \ldots , x_m \}$, the goal is to generate images from the dataset. The inputs are a finite set of numbers $\alpha_t , t \in T$ and a finite set of noise vectors $\epsilon_j$ for $j \in J$.

The outputs are a set of models, $g_t : \mathcal{X} \rightarrow \mathcal{X}$, $g_t (x) = g(x, t)$ for $t \in T$.

For each $t \in T$ , let $S_t = \{x_{t,i,j}\}, i = 1, \ldots , m$ for $j \in J$ be the dataset of t-noisy images, $x_{t,i,j} = \sqrt{\alpha_t} x_i + \sqrt{1 - \alpha_t} \epsilon_j$

The training loss for $g_t$ is given by
\begin{equation*}
\hat{L}_{S_t}(g_t) = \frac{1}{m} \sum_{i=1}^{m} \sum_{j \in J} ||g_t (x_{t,i,j} ) - \epsilon_j ||^2
\end{equation*}

Define the closest point map
\begin{equation*}
  g^{CP}(x) = g^{CP}(x, S^m) = \argmin_{x_i \in S^m} ||x - x_i||^2
\end{equation*}

\subsubsection*{Exercise 4.2 (a)}
Assume that, for each $x_{t,i,j} \in S_t$, $g^{CP}(x_{t,i,j}) = x_i$. Find the loss minimizer $g^*_t$ and show that the loss is zero.

\subsubsection*{Solution to Exercise 4.2 (a)}
In this argument we will show that under the given assumption, the noise vector $\epsilon_j$ can be retrieved from $x_{t,i,j}$, and this fact can be used to construct a loss minimizing function.

The training loss is the mean squared error between the output of $g_t(x_{t,i,j})$ and the noise vectors $\epsilon_j$:

\begin{equation*}
  \hat{L}_{S_t}(g_t) = \frac{1}{m} \sum_{i=1}^{m} \sum_{j \in J} ||g_t (x_{t,i,j} ) - \epsilon_j ||^2
\end{equation*}

Minimizing this loss amounts to minimizing $||g_t (x_{t,i,j})-\epsilon_j||^2$ for every $i = 1, ..., m$, and every $j \in J$.

Since $x_{t,i,j} = \sqrt{\alpha_t}x_i + \sqrt{1-\alpha_t}\epsilon_j$, rearranging for $\epsilon_j$ we can construct a new function $g^*$ which will depend only on $x_{t,i,j}$ and $t$:

\begin{align*}
  \epsilon_j &= \frac{x_{t,i,j} - \sqrt{\alpha_t}x_i}{\sqrt{1-\alpha_t}} \\
   &= \frac{x_{t,i,j} - \sqrt{\alpha_t} \cdot \argmin_{x_k \in S^m} ||x_{t,i,j}-x_k||^2}{\sqrt{1-\alpha_t}} \\
  &= g^*_t(x_{t,i,j})
\end{align*}

Where we take advantage of the fact that $g^{CP}(x_{t,i,j}) = x_i$ to remove the dependency on $x_i$. By construction $g$ depends only on $x_{t,i,j}$ and $t$. This means $g^*_t(x)$ is a viable candidate minimizer for the loss $\hat{L}_{S_t}$.

Lastly we show that $g^*_t$ minimizes $\hat{L}_{S_t}$:

\begin{align*}
  \hat{L}_{S_t}(g^*_t) &= \frac{1}{m} \sum_{i=1}^{m} \sum_{j \in J} ||g^*_t (x_{t,i,j} ) - \epsilon_j ||^2 \\
  &= \frac{1}{m} \sum_{i=1}^{m} \sum_{j \in J} ||\epsilon_j - \epsilon_j ||^2 \\
  &= \frac{1}{m} \sum_{i=1}^{m} \sum_{j \in J} 0 = 0
\end{align*}

Since the loss is non-negative, we can conclude that $g^*_t$ minimizes the loss.

\subsubsection*{Exercise 4.2 (b)}
In the case above, are there any other loss minimizers?

\subsubsection*{Solution to Exercise 4.2 (b)}

Suppose there are two different minimizers $g^t$ and $h^t$ of the loss. They both achieve the minimum loss, which is zero. So for all $i = 1, ..., m$ and for all $j \in J$,

\begin{equation*}
  ||g^t(x_{t,i,j}) - \epsilon_j||^2 = ||h^t(x_{t,i,j}) - \epsilon_j||^2 = 0
\end{equation*}

This implies that $g^t(x_{t,i,j}) = h^t(x_{t,i,j}) = \epsilon_j$ for all $i$ and $j$. Therefore, if $g^t$ and $h^t$ are both loss minimizers for $\hat{L}_{S^t}$, they must be equal. Therefore $g^*_t$ is unique.

\subsubsection*{Exercise 4.2 (c)}
How do the loss minimizers for this generative model compare to the loss minimizers for GANs? Which is better in terms of coverage of the dataset?

\subsubsection*{Solution to Exercise 4.2 (c)}

In GANs, the generator aims to fool the discriminator by producing outputs that are indistinguishable from the real data, thus the loss minimizer corresponds to a generator that perfectly replicates the real data distribution. However, GANs can suffer from mode collapse, where they only generate a small subset of the real data, failing to cover the entire data distribution.

In contrast, diffusion generative add noise to the real data and learns to reverse this process, mapping noised images back to the originals. In this case, the loss minimizer is a deterministic function that directly maps the noisy data back to the closest point in the original dataset. This approach virtually guarantees coverage over the original data, as long as the original images are not too close together.

In terms of dataset coverage, diffusion generative models generally perform better. They more directly address mode collapse, which is a notable issue with GANs.

\newpage

\subsection*{Exercise 4.3 (Normalizing Flows: transformation of densities)}

All problems in this section are drawn from ``Understanding Deep Learning'' \citet{prince2023understanding}.

\subsubsection*{Problem 16.1}

Consider transforming a uniform base density defined on $z\in[0,1]$ using the function $x = f[z] = z^2$. Find an expression for the transformed distribution $Pr(x)$.

\subsubsection*{Problem 16.1 solution}

Let $P_Z$ be the probability density of $z$, then $P_Z = 1$, since $z\sim U[0,1]$.

Now let $x = z^2$, then $z = \sqrt{x}$, and $\frac{dz}{dx} = \frac{1}{2\sqrt{x}}$. Let $P_X$ be the probability density of $x$. Lastly we use the change of variables formula for probability densities:

\begin{align*}
  p_X(x) &= p_Z(z) \left|\frac{dz}{dx}\right| \\
  &= 1 \cdot \left| \frac{1}{2 \sqrt{x}} \right| = \frac{1}{2\sqrt{x}}
\end{align*}

\subsubsection*{Problem 16.2}

Consider transforming a standard normal distribution:

\begin{equation*}
  Pr(z) = \frac{1}{\sqrt{2\pi}} \exp \left[\frac{-z^2}{2}\right]
\end{equation*}

with the function:

\begin{equation*}
  x = f[z] = \frac{1}{1 + \exp[-z]}
\end{equation*}

Find an expression for the transformed distribution $Pr(x)$.

\subsubsection*{Problem 16.2 solution}

As before, let $P_Z = Pr(z)$. Then $x = f[z]$ implies $z = \log(x(1-x))$, from which we derive: $\frac{dz}{dx} = \frac{1-2x}{x(1-x)}$

Then using the change of variables formula from before:

\begin{align*}
  p_X(x) &= p_Z(z) \left|\frac{dz}{dx}\right| \\
  &= \frac{1}{\sqrt{2\pi}} \exp \left[-\frac{1}{2}(\log(x(1-x)))^2 \right] \cdot \frac{|1-2x|}{x(1-x)}
\end{align*}

The expression looks a bit nasty but I don't think it's possible to simplify further.

\newpage

\subsection*{Exercise 4.4 (Normalizing Flows: Fixed point theorem)}

\subsubsection*{Problem 16.11}

Write out the expression for the KL divergence in the equation:

\begin{equation*}
  \phi = \argmin \left[ KL \left[ \sum_{i=1}^I \delta[x-f[z_i,\phi]] || q(x) \right] \right]
\end{equation*}

Why does it not matter if we can only evaluate the probability q(x) up to a scaling factor $\kappa$? Does the network have to be invertible to minimize this loss function? Explain your reasoning.

\subsection*{Exercise 4.5 (Diffusion generative models)}

\subsubsection*{Problem 18.1}

Show that if $Var[x_{t-1}] = \textbf{I}$ and we use the update:

\begin{equation*}
  x_t = \sqrt{1-\beta_t}\cdot x_{t-1} + \sqrt{\beta_t} \cdot \epsilon_t,
\end{equation*}

then $Var[x_t] = \textbf{I}$ so the variance stays the same.

\subsubsection*{Problem 18.2}

Consider the variable:

$$z = a \cdot \epsilon_1 + b \cdot \epsilon_2$$

where both $\epsilon_1$ and $\epsilon_2$ are drawn from independent standard normal distributions with mean zero and unit variance. Show that:

\begin{align*}
  \mathbb{E}[z] &= 0 \\
  Var[z] &= a^2 + b^2
\end{align*}

so we could equivalently compute $z = \sqrt{a^2 + b^2} \cdot \epsilon$ where $\epsilon$ is also drawn from a standard normal distribution.

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliography{sample.bib} % The file containing the bibliography


\end{document}
