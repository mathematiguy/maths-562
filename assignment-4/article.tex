%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Language reclamation: Literature review}} % The article title

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Caleb Moses*}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

\title{MATH/COMP 562 ASSIGNMENT 4}
\date{Due: April 9th (Sunday)}
\author{Adam M. Oberman}
\maketitle

\section*{Generative Models}
Refer to \url{https://udlbook.github.io/udlbook/}

\subsection*{Exercise 4.1 (GANs)}

Given a dataset of genuine images, $S_m = \{x_1, \ldots, x_m\}$. The goal is to train a generator, $x_i = g(z_i, \theta)$, to generate images $x_i \in S_m$ from noise $z_i$. This is achieved by also training a discriminator, $f(x, \phi)$, which classifies an image as genuine or generated, using the minimax training loss
\begin{equation*}
\max_\theta \min_\phi \mathcal{L}^S_\theta(f(x, \phi), y)
\end{equation*}
applied to the combined dataset, $S_\theta = S_N \cup S_G^\theta$,
where
\begin{itemize}
    \item $S_N = \{(x_1, 1), \ldots, (x_m, 1)\}$, the (fixed) genuine images, with label $y = 1$,
    \item $S_G^\theta = \{(x^\theta_1, 0), \ldots, (x^\theta_m, 0)\}$, a changing set of generated images $x^\theta_i = g(z_i, \theta)$, with label $y = 0$.
\end{itemize}

\subsubsection*{Explicit GAN loss}

From \citet{prince2023understanding} the GAN loss function is as follows

%% \begin{equation*}
%% \hat{\theta} = \textrm{argmax}_\theta \left[ \min_\phi \left[ \sum \right] \right]
%% \end{equation*}

\subsubsection*{Exercise 4.1 (a)}
Write down and explain the loss for the discriminator, if the generator is fixed.

\subsubsection*{Solution 4.1 (a)}
For a fixed generator, we can imagine

Assuming the generator is fixed, the discriminator's goal is to correctly classify the genuine images and the generated images. The loss for the discriminator is a binary cross-entropy loss function, which measures the error of a classification model at two-class classification tasks. This loss increases as the predicted probability diverges from the actual label. The binary cross-entropy loss can be expressed as follows:

\begin{equation}
L(y,\hat{y}) = - \frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y_i}) + (1-y_i) \log(1-\hat{y_i})]
\end{equation}

where $y$ is the true label, $\hat{y}$ is the predicted label, and $N$ is the number of samples.

\subsubsection*{Exercise 4.1 (b)}
Write down and explain the loss (or gain, since it is a maximization) for the generator, if the discriminator is fixed.

\subsubsection*{Solution 4.1 (b)}
The generator's loss function is also the binary cross-entropy loss function. However, the goal is the opposite, i.e., it tries to maximize this loss. The generator wants the discriminator to incorrectly classify its generated images as genuine. Thus, when the discriminator is fixed, the generator tries to generate images that look more like the genuine images to trick the discriminator.

\subsubsection*{Exercise 4.1 (c)}
Characterize the saddle points of the loss. If the generator outputs the full set of genuine images, is this optimal? What if the generator outputs just a subset of the genuine images?

A saddle point, $(\theta, \phi)$, is a point where (i) if we fix $\phi$ and change $\theta$, we cannot further increase the loss, and (ii) if we fix $\theta$ and change $\phi$, we cannot further decrease the loss.

\subsubsection*{Solution 4.1 (c)}
Saddle points are the points where the gradient of the loss function equals zero, i.e., the function is neither entirely convex nor entirely concave. If the generator outputs the full set of genuine images, this is not optimal, as the generator is not adding any new information. Similarly, if the generator only outputs a subset of the genuine images, it may not be optimizing the diversity of the generated images, which is also not optimal.

\subsection*{Exercise 4.2 (Diffusion generative models)}
Given the dataset $S_m = \{x_1 , \ldots , x_m \}$, the goal is to generate images from the dataset. The inputs are a finite set of numbers $\alpha_t , t \in T$ and a finite set of noise vectors $\zeta_j , j \in J$.

The outputs are a set of models, $g_t : X \rightarrow X$, $g_t (x) = g(x, t), t \in T$.

For each $t \in T$ , let $S_t = \{x_{t,i,j}\}, i = 1, \ldots , m, j \in J$ be the dataset of t-noisy images, $x_{t,i,j} = \sqrt{\alpha_t} x_i + \sqrt{1 - \alpha_t} \zeta_j$

The training loss for $g_t$ is given by
\begin{equation*}
\mathcal{L} S_t (g_t ) = \frac{1}{m} \sum_{i=1}^{m} \sum_{j \in J} ||g_t (x_{t,i,j} ) - \zeta_j ||^2
\end{equation*}

Define the closest point map
\begin{equation*}
g_{CP}(x) = g_{CP} (x, S_m ) = \arg \min_{x_i \in S_m} ||x - x_i ||^2
\end{equation*}

\subsubsection*{Exercise 4.2 (a)}
Assume that, for each $x_{t,i,j} \in S_t$, $g_{CP}(x_{t,i,j}) = x_i$. Find the loss minimizer $g^*_t$ and show that the loss is zero.

\subsubsection*{Solution to Exercise 4.2 (a)}
This requires a specific calculation or a specific dataset to answer more accurately. The answer might be based on the characteristics of the dataset and the model's architecture.

\subsubsection*{Exercise 4.2 (b)}
In the case above, are there any other loss minimizers?

\subsubsection*{Solution to Exercise 4.2 (b)}
This question also requires more specific details about the dataset or the model's architecture to provide a complete and accurate answer.

\subsubsection*{Exercise 4.2 (c)}
How do the loss minimizers for this generative model compare to the loss minimizers for GANs? Which is better in terms of coverage of the dataset?

\subsubsection*{Solution to Exercise 4.2 (c)}
The comparison between the loss minimizers for this generative model and the loss minimizers for GANs is largely dependent on the specific characteristics of the models and the dataset.

\subsection*{Exercise 4.3 (Normalizing Flows: transformation of densities)}
Problems 16.1 and 16.2 from ”Understanding Deep Learning”

\subsection*{Exercise 4.4 (Normalizing Flows: Fixed point theorem)}
Problem 16.11 from Chapter 16 of ”Understanding Deep Learning”

\subsection*{Exercise 4.5 (Diffusion generative models)}
Problems 18.1 and 18.2 from Chapter 18 of ”Understanding Deep Learning”

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliography{sample.bib} % The file containing the bibliography


\end{document}
