%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Language reclamation: Literature review}} % The article title

%\subtitle{Subtitle} % Uncomment to display a subtitle

\author{\spacedlowsmallcaps{Caleb Moses*}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

\title{MATH/COMP 562 ASSIGNMENT 4}
\date{Due: April 9th (Sunday)}
\author{Adam M. Oberman}
\maketitle

\section*{Generative Models}
Refer to \url{https://udlbook.github.io/udlbook/}

\subsection*{Exercise 4.1 (GANs)}

Given a dataset of genuine images, $S_m = \{x_1, \ldots, x_m\}$. The goal is to train a generator, $x_i = g(z_i, \theta)$, to generate images $x_i \in S_m$ from noise $z_i$. This is achieved by also training a discriminator, $f(x, \phi)$, which classifies an image as genuine or generated, using the minimax training loss
\begin{equation*}
\max_\theta \min_\phi \mathcal{L}^S_\theta(f(x, \phi), y)
\end{equation*}
applied to the combined dataset, $S_\theta = S_N \cup S_G^\theta$,
where
\begin{itemize}
    \item $S_N = \{(x_1, 1), \ldots, (x_m, 1)\}$, the (fixed) genuine images, with label $y = 1$,
    \item $S_G^\theta = \{(x^\theta_1, 0), \ldots, (x^\theta_m, 0)\}$, a changing set of generated images $x^\theta_i = g(z_i, \theta)$, with label $y = 0$.
\end{itemize}

%% \subsubsection*{Explicit GAN loss}

%% From \citet{prince2023understanding} the GAN loss function is as follows:

%% %% This is the characterisation of the saddle points
%% \begin{equation*}
%% \hat{\theta} = \textrm{argmax}_\theta \left[ \min_\phi \left[ \sum_j - \log(1 - D(G(z_j, \theta), \phi))) - \sum_i \log(D(x_i, \phi))) \right] \right]
%% \end{equation*}

%% where $\theta$ represents the weights of the generator function $G$, and $\phi$ is the weights of the discriminator $D$, and $\sigma$ is the sigmoid logistic function.

\subsubsection*{Exercise 4.1 (a)}
Write down and explain the loss for the discriminator, if the generator is fixed.

\subsubsection*{Solution 4.1 (a)}

From \citet{prince2023understanding} 15.5 the discriminator loss function is given by:

\begin{equation*}
  L(\phi) = - \sum_j \log\left(1 - \textrm{sig}(f(g(z_j, \theta), \phi)\right) - \sum_i log\left(\textrm{sig}(f(x_i, \phi))\right)
\end{equation*}

where ``$\textrm{sig}$'' represents the sigmoid function.

For a fixed generator, we can imagine each $g(z_i, \theta) = \hat{x}_i$ where $\hat{x}_i$ is an image generated according to $g$ which approximates $x_i$. We can simplify further if we let $D(x, \phi) = \textrm{sig}(f(x, \phi))$, basically considering the sigmoid classification layer as part of the discriminator function.

This gives us the loss function below:

\begin{equation*}
  L(\phi) = - \sum_j \log(1 - D(\hat{x}_j, \phi)) - \sum_i log(D(x_i, \phi))
\end{equation*}

The result is a binary cross-entropy loss where the goal is to distinguish $x_i$ from $\hat{x}_i$. Real samples $x_i$ decrease the loss by $log(D(x_i, \phi))$ while generated samples increase the loss by $\log(1-D(\hat{x}_j, \phi))$.

\subsubsection*{Exercise 4.1 (b)}
Write down and explain the loss (or gain, since it is a maximization) for the generator, if the discriminator is fixed.

\subsubsection*{Solution 4.1 (b)}

From \citet{prince2023understanding} 15.5 the generator gain function is given by:

\begin{equation*}
  L(\theta) = - \sum_j \log\left(1 - \textrm{sig}(f(g(z_j, \theta), \phi)\right)
\end{equation*}

where, as before ``$\textrm{sig}$'' represents the sigmoid function.

For a fixed discriminator, we can let $D_\phi(x) = \textrm{sig}(f(x, \phi))$ (we are bringing the $\phi$ into the function, since now it can't vary). In this scenario the GAN problem requires us to find the optimal $\hat{\theta}$ satisfying below:

\begin{equation*}
  L(\theta) = - \sum_j \log(1 - D_{\phi}(g(z_j, \theta)))
\end{equation*}

The purpose of the generator gain function is to reward the generator for samples that are assigned high probability by the discriminator.

\subsubsection*{Exercise 4.1 (c)}
Characterize the saddle points of the loss. If the generator outputs the full set of genuine images, is this optimal? What if the generator outputs just a subset of the genuine images?

A saddle point, $(\theta, \phi)$, is a point where (i) if we fix $\phi$ and change $\theta$, we cannot further increase the loss, and (ii) if we fix $\theta$ and change $\phi$, we cannot further decrease the loss.

\subsubsection*{Solution 4.1 (c)}
Saddle points are the points where the gradient of the loss function equals zero, i.e., the function is neither entirely convex nor entirely concave. If the generator outputs the full set of genuine images, this is not optimal, as the generator is not adding any new information. Similarly, if the generator only outputs a subset of the genuine images, it may not be optimizing the diversity of the generated images, which is also not optimal.

\subsection*{Exercise 4.2 (Diffusion generative models)}
Given the dataset $S_m = \{x_1 , \ldots , x_m \}$, the goal is to generate images from the dataset. The inputs are a finite set of numbers $\alpha_t , t \in T$ and a finite set of noise vectors $\zeta_j , j \in J$.

The outputs are a set of models, $g_t : X \rightarrow X$, $g_t (x) = g(x, t), t \in T$.

For each $t \in T$ , let $S_t = \{x_{t,i,j}\}, i = 1, \ldots , m, j \in J$ be the dataset of t-noisy images, $x_{t,i,j} = \sqrt{\alpha_t} x_i + \sqrt{1 - \alpha_t} \zeta_j$

The training loss for $g_t$ is given by
\begin{equation*}
\mathcal{L} S_t (g_t ) = \frac{1}{m} \sum_{i=1}^{m} \sum_{j \in J} ||g_t (x_{t,i,j} ) - \zeta_j ||^2
\end{equation*}

Define the closest point map
\begin{equation*}
g_{CP}(x) = g_{CP} (x, S_m ) = \arg \min_{x_i \in S_m} ||x - x_i ||^2
\end{equation*}

\subsubsection*{Exercise 4.2 (a)}
Assume that, for each $x_{t,i,j} \in S_t$, $g_{CP}(x_{t,i,j}) = x_i$. Find the loss minimizer $g^*_t$ and show that the loss is zero.

\subsubsection*{Solution to Exercise 4.2 (a)}
This requires a specific calculation or a specific dataset to answer more accurately. The answer might be based on the characteristics of the dataset and the model's architecture.

\subsubsection*{Exercise 4.2 (b)}
In the case above, are there any other loss minimizers?

\subsubsection*{Solution to Exercise 4.2 (b)}
This question also requires more specific details about the dataset or the model's architecture to provide a complete and accurate answer.

\subsubsection*{Exercise 4.2 (c)}
How do the loss minimizers for this generative model compare to the loss minimizers for GANs? Which is better in terms of coverage of the dataset?

\subsubsection*{Solution to Exercise 4.2 (c)}
The comparison between the loss minimizers for this generative model and the loss minimizers for GANs is largely dependent on the specific characteristics of the models and the dataset.

\subsection*{Exercise 4.3 (Normalizing Flows: transformation of densities)}
Problems 16.1 and 16.2 from ”Understanding Deep Learning”

\subsection*{Exercise 4.4 (Normalizing Flows: Fixed point theorem)}
Problem 16.11 from Chapter 16 of ”Understanding Deep Learning”

\subsection*{Exercise 4.5 (Diffusion generative models)}
Problems 18.1 and 18.2 from Chapter 18 of ”Understanding Deep Learning”

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliography{sample.bib} % The file containing the bibliography


\end{document}
